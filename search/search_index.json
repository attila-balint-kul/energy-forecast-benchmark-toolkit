{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Energy  Forecast Benchmark Toolkit","text":"<p>Energy Forecast Benchmark Toolkit is a Python project that aims to provide common tools to benchmark forecast models.</p>","tags":["example"]},{"location":"#installation","title":"Installation","text":"<p>Use the package manager pip to install foobar.</p> <pre><code>pip install enfobench\n</code></pre>","tags":["example"]},{"location":"#usage","title":"Usage","text":"<p>Load your own data and create a dataset.</p> <pre><code>import pandas as pd\n\nfrom enfobench import Dataset\n\n# Load your datasets\ndata = pd.read_csv(\"../path/to/your/data.csv\", parse_dates=['timestamp'], index_col='timestamp')\n\n# Create a target DataFrame that has a pd.DatetimeIndex and a column named 'y'\ntarget = data.loc[:, ['target_column']].rename(columns={'target_column': 'y'})\n\n# Add covariates that can be used as past covariates. This also has to have a pd.DatetimeIndex\npast_covariates = data.loc[:, ['covariate_1', 'covariate_2']]\n\n# As sometimes it can be challenging to access historical forecasts to use future covariates, \n# the package also has a helper function to create perfect historical forecasts from the past covariates.\nfrom enfobench.datasets.utils import create_perfect_forecasts_from_covariates\n\n# The example below creates simulated perfect historical forecasts with a horizon of 24 hours and a step of 1 day.\nfuture_covariates = create_perfect_forecasts_from_covariates(\n    past_covariates,\n    horizon=pd.Timedelta(\"24 hours\"),\n    step=pd.Timedelta(\"1 day\"),\n)\n\ndataset = Dataset(\n    target=target,\n    past_covariates=past_covariates,\n    future_covariates=future_covariates,\n)\n</code></pre> <p>The package integrates with the HuggingFace Dataset 'EDS-lab/electricity-demand'.  To use this, just download all the files from the data folder to your computer.</p> <pre><code>from enfobench import Dataset\nfrom enfobench.datasets import ElectricityDemandDataset\n\n# Load the dataset from the folder that you downloaded the files to.\nds = ElectricityDemandDataset(\"/path/to/the/dataset/folder/that/contains/all/subsets\")\n\n# List all meter ids\nds.list_unique_ids()\n\n# Get dataset for a specific meter id\ntarget, past_covariates, metadata = ds.get_data_by_unique_id(\"unique_id_of_the_meter\")\n\n# Create a dataset\ndataset = Dataset(\n    target=target,\n    past_covariates=past_covariates,\n    future_covariates=None,\n    metadata=metadata\n)\n</code></pre> <p>You can perform a cross validation on any model locally that adheres to the <code>enfobench.Model</code> protocol.</p> <pre><code>import MyModel\nimport pandas as pd\nfrom enfobench.evaluation import cross_validate\n\n# Import your model and instantiate it\nmodel = MyModel()\n\n# Run cross validation on your model\ncv_results = cross_validate(\n    model,\n    dataset,\n    start_date=pd.Timestamp(\"2018-01-01T00:00:00\"),\n    end_date=pd.Timestamp(\"2018-01-31T00:00:00\"),\n    horizon=pd.Timedelta(\"24 hours\"),\n    step=pd.Timedelta(\"1 day\"),\n)\n</code></pre> <p>You can use the same crossvalidation interface with your model served behind an API.  To make this simple, both a client and a server are provided.</p> <pre><code>import pandas as pd\nfrom enfobench.evaluation import cross_validate, ForecastClient\n\n# Import your model and instantiate it\nclient = ForecastClient(host='localhost', port=3000)\n\n# Run cross validation on your model\ncv_results = cross_validate(\n    client,\n    dataset,\n    start_date=pd.Timestamp(\"2018-01-01\"),\n    end_date=pd.Timestamp(\"2018-01-31\"),\n    horizon=pd.Timedelta(\"24 hours\"),\n    step=pd.Timedelta(\"1 day\"),\n)\n</code></pre> <p>The package also collects common metrics used in forecasting.</p> <pre><code>from enfobench.evaluation import evaluate_metrics\n\nfrom enfobench.evaluation.metrics import (\n    mean_bias_error,\n    mean_absolute_error,\n    mean_squared_error,\n    root_mean_squared_error,\n)\n\n# Simply pass in the cross validation results and the metrics you want to evaluate.\nmetrics = evaluate_metrics(\n    cv_results,\n    metrics={\n        \"mean_bias_error\": mean_bias_error,\n        \"mean_absolute_error\": mean_absolute_error,\n        \"mean_squared_error\": mean_squared_error,\n        \"root_mean_squared_error\": root_mean_squared_error,\n    },\n)\n</code></pre> <p>In order to serve your model behind an API, you can use the built-in server factory.</p> <pre><code>import uvicorn\nfrom enfobench.evaluation.server import server_factory\n\nmodel = MyModel()\n\n# Create a server that serves your model\nserver = server_factory(model)\nuvicorn.run(server, port=3000)\n</code></pre>","tags":["example"]},{"location":"#benchmarking","title":"Benchmarking","text":"<p>The package also provides a benchmarking framework that can be used to benchmark your model against other models. </p> <p>The results of the benchmarking are openly accessible here.</p>","tags":["example"]},{"location":"#contributing","title":"Contributing","text":"<p>Contributions and feedback are welcome! For major changes, please open an issue first to discuss what you would like to change.</p> <p>If you'd like to contribute to the project, please follow these steps:</p> <p>Fork the repository. Create a new branch for your feature or bug fix. Make your changes and commit them. Push your changes to your forked repository. Submit a pull request describing your changes.</p>","tags":["example"]},{"location":"#license","title":"License","text":"<p>BSD 2-Clause License</p>","tags":["example"]},{"location":"examples/","title":"Examples","text":"<p>This repository contains example models and notebooks to get started with the benchmark toolkit. The examples models are found in the <code>models/</code> folder, and the example notebooks are in the <code>notebooks/</code> folder.</p>"},{"location":"examples/#folder-structure","title":"Folder Structure","text":"<p>The repository follows this structure:</p> <pre><code>\u251c\u2500\u2500 data\n\u2502   \u2514\u2500\u2500 electricity-demand          &lt;- Electricity demand dataset.\n\u2502       \u251c\u2500\u2500 demand.parquet          &lt;- Demand data subset.\n\u2502       \u251c\u2500\u2500 metadata.parquet        &lt;- Metadata subset.\n\u2502       \u2514\u2500\u2500 weather.parquet         &lt;- Weather data subset.\n\u2502\n\u251c\u2500\u2500 models                          &lt;- Example models each in its own subfolder.\n\u2502   \u251c\u2500\u2500 sf-naive-seasonal           &lt;- Naive seasonal model based on statsforecast package.\n\u2502   \u2502   \u251c\u2500\u2500 src                     &lt;- Source code for the model.\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 main.py             &lt;- Entrypoint for the forecast server.\n\u2502   \u251c\u2500\u2500 Dockerfile                  &lt;- Example Dockerfile for the model. \n\u2502   \u2514\u2500\u2500 requirements.txt            &lt;- Model's requirements.\n\u2502\n\u251c\u2500\u2500 notebooks                       &lt;- Jupyter notebooks, should be read in order.\n\u2502   \u251c\u2500\u2500 01. Univariate.ipynb        &lt;- Simple univariate forecast model benchmarking example.\n\u2502   \u251c\u2500\u2500 02. Multivariate.ipynb      &lt;- Multivariate forecast model benchmarking example.\n\u2502   \u2514\u2500\u2500 03. ForecastClient.ipynb    &lt;- Benchmarking using the ForecastClient example.\n\u2502\n\u2514\u2500\u2500 README.md                       &lt;- The top-level README for getting started.\n</code></pre>"},{"location":"examples/#requirements","title":"Requirements","text":"<p>To contribute models to the benchmark, you need to have Docker installed.  Follow the installation procedure for your platform on the docker website.</p>"},{"location":"examples/#getting-started","title":"Getting Started","text":"<p>Clone this repository: <pre><code>git clone https://github.com/attila-balint-kul/energy-forecast-benchmark-toolkit\ncd energy-forecast-benchmark-toolkit\n</code></pre></p> <p>Install the requirements (recommended inside a virtual environment): <pre><code>pip install notebook enfobench\n</code></pre></p> <p>To run the notebooks, you also need the HuggingFace dataset EDS-lab/electricity-demand. Download all three files from the <code>data/</code> folder into the <code>data/electicity-demand/</code> folder of this repository.</p> <p>Run the example notebooks in the <code>notebooks</code> folder.</p>"},{"location":"examples/#creating-a-model","title":"Creating a Model","text":"<p>To create a model, use the <code>models/sf-naive/</code> folder as a template.  If you follow the folder structure, have a <code>requirements.txt</code> file,  and all your source code is inside the <code>src/</code> folder, there is generally  no need to change the <code>Dockerfile</code>. Once your model is ready, you can build the docker image:</p> <pre><code>docker build -t tag-that-identifies-the-model ./path/to/the/folder/containing/the/Dockerfile\n</code></pre> <p>To run the Docker image: <pre><code>docker run -p 3000:3000 tag-that-identifies-the-model\n</code></pre></p> <p>Then you can test your model by using the <code>03. ForecastClient.ipynb</code> notebook.</p> <p>Once the model is tested, push it to any public Docker registry  (e.g., DockerHub). Contact us with the repository and model tag,  and we will add it to the dashboard.</p>"},{"location":"tags/","title":"Tags","text":"<p>Following is a list of relevant tags:</p>"},{"location":"tags/#tag:example","title":"example","text":"<ul> <li>            Home          </li> </ul>"}]}